---
title: L1正则化和L2正则化
author: zhanghao
categories:
- 机器学习
- 优化问题
tags: 
- L1正则化和L2正则化简介 
date: 2020-12-21 
excerpt: L1正则化和L2正则化简介
mathjax: false
---

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": { 
        preferredFont: "TeX", 
        availableFonts: ["STIX","TeX"], 
        linebreaks: { automatic:true }, 
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) 
    },
    tex2jax: { 
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ], 
        processEscapes: true, 
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {  
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, 
        Macros: { href: "{}" } 
    },
    messageStyle: "none"
    }); 
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

# 前言

机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作ℓ1​-norm 和 ℓ2​-norm，中文称作 L1正则化 和 L2正则化，或者 L1范数 和 L2范数。

L1正则化和L2正则化可以看做是损失函数的惩罚项。所谓『惩罚』是指对损失函数中的某些参数做一些限制。对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）。

将正则化项加入到损失函数中，我们在优化过程中就会连带正则化项一起优化。这样我们就会对某些参数作出限制，从而避免模型的过拟合和过度复杂化。
# L1正则化和L2正则化

假设我们有一个平方差损失函数，为了防止过拟合我们分别添加L1正则和L2正则。

L1正则化：
$$y=\min_{w}\frac{1}{2n}{||Xw-y||}\_{2}^{2}+\alpha{||w||}\_{1}$$

L2正则化：

$$y=\min_{w}\frac{1}{2n}{||Xw-y||}\_{2}^{2}+\alpha{||w||}\_{2}^{2}$$

一般回归分析中w表示特征的系数，从上式可以看到正则化项是对系数做了处理（限制）。L1正则化和L2正则化的说明如下：
    
L1正则化是指权值向量w中各个元素的绝对值之和：

$$ℓ1​-norm=|w_{1}|+...+|w_{n}|$$

通常表示为: 

$$ℓ1​-norm=||w||_{1}$$

L2正则化是指权值向量w中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号）:

$$ℓ2​-norm=\sqrt{w_{1}^{2}+...+w_{n}^{2}}$$

通常表示为: 
$$ℓ2-norm=||w||_{2}$$

那添加L1和L2正则化有什么用？

L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择

L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合

# L1正则化和L2正则化性质的由来
添加有正则化项的损失函数具有如下形式：
$$y=\min_{w}\frac{1}{2n}{||Xw-y||}\_{2}^{2}+\alpha{||w||}\_{norm}$$
我们可以将求解这个问题变成求解带有不等式约束的优化问题，这一类优化问题的范式如下： 
$$objective:\min_{w}\frac{1}{2n}{||Xw-y||}\_{2}^{2}$$
$$s.t:{||w||}\_{norm}<=L$$
在这个优化问题中，目标函数是凸函数，满足l1正则约束条件（${||w||}\_{1}<=L$）或者l2正则约束条件（${||w||}\_{2}<=L$）的点集合是凸集合，因此目标函数​等值线与满足约束条件的点所构成的空间首次相交的地方就是最优解。
考虑二维的情况，也就是只有两个权值$w_{1}$和$w_{2}$。此时，l1约束条件成为:
$$|w_{1}|+|w_{2}|<=L$$
最优解出现的情况可以在二维平面上画出来，如下图所示：

![图1](/pic/zhanghao/l1norm/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTYwOTA0MTg0NDI4NDU5.png)
* *图1:图中的彩虹色圆圈是目标函数​的等值线，黑色方形是$|w_{1}|+|w_{2}|=L$的图形，这个函数画出来就是一个方框。*

可以看到，l1正则条件约束的边界在和每个坐标轴相交的地方都有“角”出现，而目标函数的等值线大部分时候都会在角的地方相交。可以直观想象，因为l1正则条件约束的边界有很多『突出的角』（二维情况下四个，多维情况下更多），而在这些角上，会有很多权值等于0（因为角就在坐标轴上），这就是为什么l1正则化可以产生稀疏模型，进而可以用于特征选择。

l2约束条件在二维情况下则变成：

$$\sqrt{w_{1}^{2}+w_{2}^{2}}<=L$$

最优解出现的情况可以在二维平面上画出来，如下图所示：

![图2](/pic/zhanghao/l1norm/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTYwOTA0MTg0NjQ2OTYz.png)
* *图2:图中的彩虹色圆圈是目标函数​的等值线，黑色圆形是$\sqrt{w_{1}^{2}+w_{2}^{2}}=L$的图形，这是一个以L为半径，以原点为圆心的圆。*

相比之下，l2正则条件约束的边界就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么l1正则能产生稀疏性，而l2正则不行的原因了。

l2正则为什么能防止过拟合呢？

假设线性回归的损失函数为：
$$J(w)=\frac{1}{2n}\sum_{i=1}^{n}(h_{w}(x^{(i)})-y^{(i)})^{2}$$
其中，$h_{w}(x)=w_{0}x_{0}+w_{1}x_{1}+...+w_{n}x_{n}$，在采用梯度下降算法更新参数时，对参数$w_{i}$求导：
$$\frac{\partial }{\partial{w_{i}}}J(w_{i}) = \frac{1}{n}\sum_{i=1}^{n}(h_{w}(x^{(i)})-y^{(i)})x^{(i)}$$
参数更新公式为,其中$\alpha$为学习率：
$$w_{i}: = w_{i}-\alpha \frac{1}{n}\sum_{i=1}^{n}(h_{w}(x^{(i)})-y^{(i)})x^{(i)}$$
在添加了l2正则项之后，参数更新公式为,其中$\alpha$为学习率：
$$w_{i}: = w_{i}(1-\alpha \frac{\lambda }{n})-\alpha \frac{1}{n}\sum_{i=0}^{n}(h_{w}(x^{(i)})-y^{(i)})x^{(i)}$$

其中$\lambda $就是正则化参数。从上式可以看到，与未添加l2正则化的迭代公式相比，每一次迭代参数都要乘以一个小于1的因子，因此在迭代过程中参数的值是不断减小的。

