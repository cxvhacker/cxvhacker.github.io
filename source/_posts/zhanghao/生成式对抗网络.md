---
title: 生成式对抗网络简介
author: zhanghao
categories:
- 机器学习
- 神经网络
tags: 
- 生成模型简介 
date: 2020-09-23 
excerpt: 简单介绍生成式对抗网络(Generative Adversarial Networks，GANs)和一些相关的有趣项目

mathjax: false
---

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": { 
        preferredFont: "TeX", 
        availableFonts: ["STIX","TeX"], 
        linebreaks: { automatic:true }, 
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) 
    },
    tex2jax: { 
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ], 
        processEscapes: true, 
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {  
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, 
        Macros: { href: "{}" } 
    },
    messageStyle: "none"
    }); 
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

# 前言
生成式对抗网络(Generative Adversarial Networks，GANs),听起来有些拗口的名字。但其实这个名字已经把这种神经网络结构的核心阐明了：就是生成和对抗。

怎么生成？谁和谁对抗？为什么使用复数Networks而不是Network？下面的内容将会揭开这些秘密。

先来揭晓最后一个问题的答案，为什么是Networks而不是Network？原因很简单，因为本来就是两个神经网络构成的生成式对抗网络。以图像模型举例,我们有一个图片生成模型（generator），它的目标是生成一张真实的图片。与此同时我们有一个图像判别模型（discriminator），它的目标是能够正确判别一张图是生成出来的还是真实存在的。

再来揭晓第二个问题答案，谁跟谁对抗？既然有两个网络，自然是生成模型（generator）和判别模型（discriminator）之间对抗。

那第一个问题的答案也就呼之欲出了，怎么生成？就是在对抗中生成啊，生成的模式就是这样：生成模型生成一些图片->判别模型学习区分生成的图片和真实图片->生成模型根据判别模型改进自己，生成新的图片->····

下面我将详细介绍一下GANs模型的结构和利用GANs实现的有趣项目和在线小工具。

# GANs简介
让我们先来用数学的语言描述生成模型和判别模型的对抗过程，就是：假设我们的生成模型是$g(z)$，其中$z$是一个随机噪声，而$g$将这个随机噪声转化为数据类型$x$，拿图片问题举例，这里$g$的输出就是一张图片。$D$是一个判别模型，对任何输入$x$，$D(x)$的输出是$0-1$范围内的一个实数，用来判断这个图片是一个真实图片的概率是多大。令$Pr$和$Pg$分别代表真实图像的分布与生成图像的分布，我们判别模型的目标函数如下：

$$\underset{G}{min}\underset{D}{max}E_{x\sim pr}[\log D(x)]+E_{z\sim pg} [\log(1- D(G(z))))]$$
这个函数我们可以分成两部分看：

　$ \underset{D}{max}E_{x\sim pr}[\log D(x)] $　这部分是含义是，我们期望判别模型能更好的分辨出真实数据也就是$x$，所以我们希望$D(x)$越接近1越好。

　$ \underset{G}{min}E_{z\sim pg} [\log(1- D(G(z))))] $　这部分是含义是，我们希望生成模型能更好的拟合真实数据也就是$x$的分布，所以我们希望$G(z)$和$x$的分布越接近越好，也就是$D(G(z))$越来觉接近1。

GANs的结构如下图所示：

![图1](/pic/zhanghao/GANs/2.png)

*图1:图中左半部分对应的是目标函数$\underset{D}{max}$的部分，右半部分对应的是目标函数的$\underset{G}{min}$的部分。*

这就像博弈中的零和游戏，生成模型和判别模型互相对抗，直到判别模型无法再分辨出需要判别的对象究竟是真实的数据，还是来自于生成模型生成的假数据。下图说明了GANs训练过程中，生成模型和判别模型的分布变化。

![图2](/pic/zhanghao/GANs/1.png)

*图2:图中的黑色散点曲线代表的是真实数据的分布，蓝色虚线是判别模型的分布，绿色实线是生成器生成的数据$G(z)$的分布。图下部的向上的箭头代表经过训练，生成模型$G$将$z$映射到了$x$的高频分布区间，当$G(z)$和$x$的分布十分接近时，判别模型$D$将无法分别数据究竟来自哪个分布，$D(x)\approx \frac{1}{2}$。*

那么如何训练GANs呢？我们采用的方法是在一个iteration中，先更新的判别模型的参数，再更新生成模型的参数。

***
*算法1 Minibatch随机梯度下降，其中$k$是超参数，代表在更新几次判别模型参数后，再去更新生成模型参数。<br>*
***
for number of training iterations do<br>
　for k steps do<br>
　　• Sample minibatch of m noise samples $ \{ z^{(1)},...,z^{(m)} \} $ from noise prior $p_g(z)$.<br>
　　• Sample minibatch of m examples $ \{ x^{(1)},...,x^{(m)} \} $ from data generating distribution $p_r(x)$<br>
　　• Update the discriminator by ascending its stochastic gradient:<br>
$$\triangledown_{\theta_d}\sum_{i=1}^{m}[\log D(x^{(i)})+\log (1-D(G(z^{(i)})))]$$
　end for<br>
　• Sample minibatch of m noise samples $ \{ z^{(1)},...,z^{(m)} \} $ from noise prior $p_g(z)$.<br>
　• Update the generator by descending its stochastic gradient:<br>
 $$\triangledown_{\theta_g}\sum_{i=1}^{m}\log (1-D(G(z^{(i)})))$$
end for<br>
***
# GANs的应用
2014年底才出现的GANs的虽然诞生的时间相对来说比较短，但是发展非常迅速，短短五年间已经出现了大规模的运用和发展。各种关于GANs的论文也层出不穷。之前很火的deepfake，也就是AI换头，以及我们日常使用的手机中（比如我自己使用vivo手机）的高清修复，消除路人等都运用到GANs。下面我向大家介绍几个有趣的GANs应用实例。

## 照片风格化
CartoonGANs，一个将照片风格化为动画场景的有趣模型<br>
论文地址：<br>
https://openaccess.thecvf.com/content_cvpr_2018/paper/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf<br>
  ![图3](/pic/zhanghao/GANs/3.png)
 
  *图3:CartoonGANs的结构，上半部分为生成模型结构，下半部分为判别模型结构。*

  将照片转换成新海诚动画风格，训练集来源于ImageNet和新海诚的动画片《云的彼端，约定的地方》，《秒速五厘米》和《言语之庭》的关键帧截图。
  下面是一些效果展示：

![real_0](/pic/zhanghao/GANs/real_0.JPG)
![cartoon_0](/pic/zhanghao/GANs/cartoon_0.jpeg)
![real_1](/pic/zhanghao/GANs/real_1.JPG)
![cartoon_1](/pic/zhanghao/GANs/cartoon_1.jpg)
![real_2](/pic/zhanghao/GANs/real_2.jpeg)
![cartoon_2](/pic/zhanghao/GANs/cartoon_2.jpeg)

## 图像超分辨率
  SRGANs(Super Resolution GANs)，将低分辨率照片转换成高分辨率的模型<br>
  论文地址：https://arxiv.org/pdf/1609.04802.pdf<br>
  ![图4](/pic/zhanghao/GANs/4.png)

  *图4:SRGANs的结构，上半部分为生成模型结构，下半部分为判别模型结构。*
  将低分辨率照片转换成高分辨率照片。下面是一些效果展示：<br>
  ![low_1](/pic/zhanghao/GANs/srgan_low1.png)
  ![high_1](/pic/zhanghao/GANs/srgan_high1.png)

## 动漫头像生成器
  基于StyleGAN训练。StyleGAN是基于样式、层层递进的神经网络。可以分离特定的样式和属性，并且对特定的样式进行混合和缩放。<br>
  论文地址：https://arxiv.org/pdf/1812.04948.pdf<br>
  生成动漫头像，并且进行样式混合。下面是一些效果展示：<br>
  ![stylegan](/pic/zhanghao/GANs/stylegan.PNG)
  
  *图5:动漫头像生成和样式混合，最上面一行和最左边一列为StyleGAN生成的动漫头像，中间是样式混合后的结果。保留最上面一行头像的脸型轮廓和发型信息，保留最左边一列头像的发色，衣物和瞳孔颜色信息。将这两种信息进行混合生成新的混合头像*

## 一些有意思的在线应用
Selfie 2 Waifu，一个将你的自拍转换成动漫角色在线应用。<br>
地址：https://waifu.lofiu.com/<br>
DeOldify，老照片，老视频修复上色。之前有一段给1905年拍摄的老北京城视频修复上色的模型就是基于这个模型。<br>
地址：https://www.myheritage.cn/incolor<br>
IllustrationGAN，将文字描述转换成图像，下面的应用是根据描述，比如发色，服装，是否戴眼镜等等生成动漫角色。<br>
地址：https://make.girls.moe/#/<br>
# 最后
GANs发展至今已经是一个非常庞大的家族，其应用也不只局限在图像处理的领域，在自然语言处理，半监督学习等等方面，GANs的应用都十分广泛。只是我知识有限，对其了解也不算深入，仅仅做一个简介，把一些有趣好玩的项目介绍给大家。如果大家有兴趣，可以在github上搜索GANs的开源项目，想必能发现更多有趣有用的知识。
# 参考文献
    1.Goodfellow I J, Pouget-Abadie J, Mirza M, et al. Generative adversarial nets[C]// International Conference on Neural Information Processing Systems. MIT Press, 2014:2672-2680.
    2.Mirza M, Osindero S. Conditional generative adversarial nets[J]. arXiv preprint arXiv:1411.1784, 2014.
    3.Radford A, Metz L, Chintala S. Unsupervised representation learning with deep convolutional generative adversarial networks[J]. arXiv preprint arXiv:1511.06434, 2015.
    4.Reed S, Akata Z, Yan X, et al. Generative adversarial text to image synthesis[J]. arXiv preprint arXiv:1605.05396, 2016.







