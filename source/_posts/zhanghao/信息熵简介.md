---
title: 信息熵简介
author: zhanghao
categories:
- 机器学习
- 信息论
tags: 
- 信息熵简介 
date: 2020-11-08 
excerpt: 介绍信息熵
mathjax: false
---

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": { 
        preferredFont: "TeX", 
        availableFonts: ["STIX","TeX"], 
        linebreaks: { automatic:true }, 
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) 
    },
    tex2jax: { 
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ], 
        processEscapes: true, 
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {  
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, 
        Macros: { href: "{}" } 
    },
    messageStyle: "none"
    }); 
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

# 前言
大家接触熵这个概念最早应该是在高中时期学习热力学三大定律的时候，在热力学中熵是无序分子运动紊乱程度的一种度量，代表物质无序运动的程度。我们也知道在没有外力的作用下，熵都是朝增加的方向移动。那么什么是信息熵呢？
# 信息熵的概念和名称的由来
信息熵的概念来源于克劳德 • 香农（Claude Shannon, 1916-2001）博士在1948年发表的论文《A mathematical theory of communications》。在这篇论文中他引进的“信息熵”概念举足轻重：它在数学上量化了通讯中“信息丢失”的本质。
香农最初并没有借用“熵”这个词汇来表达他关于信息传输中的“不确定性”的度量化，一开始他打算使用“information uncertainty”来表达这个概念，直到冯 • 诺依曼（John von Neumann, 1903-1957）给他提供建议。冯·诺依曼告诉他：就叫它熵吧，这有两个好理由。一是你的不确定性函数已在统计物理中用到过，在那里它就叫熵。第二个理由更重要：没人真正理解熵为何物，这就让你在任何时候都可能进能退，立于不败之地。
信息熵的计算公式：
$$H_{E}=-\sum_{x\in \chi }^{}p_{x}logp_{x}$$
# 信息熵公式的由来
信息熵是信息的不确定性(Uncertainty)的度量,不确定性越大,信息熵越大。

一条信息消除的不确定性越大（即熵减越大），它蕴含的信息量越大。

Shannon认为信息熵应有四大基本属性：

    1.信息熵应该是非负的

    2.信息熵应该是随概率连续变化的，且发生概率越高的事件，其所携带的信息熵越低。

    3.当所有可能选择的概率Pi相等时（即1/n）,信息熵随总数n的增加而单调增加。即事件的可能选择越多，不确定性越大。

    4.当一个选择，可以分解为两个连续选择时。分解前后的熵值应该相等，不确定性相同。

为什么会有这四个属性呢，这四个属性代表的含义是什么呢？

    一，信息熵不是个负数，不然就是什么都不做信息会自动增长。

    二，信息量是连续依赖于概率变化，就是说，某一个概率变化了很小，比如0.0001，那么这个信息量不应该变化很大。

    三，假如每一个可能的结果出现的概率一样，那么对于可能结果数量多的那个事件，新信息有更大的潜力具有更大的信息量，因为初始状态下不确定性更大。

    四，信息量和信息量之间可以相加，假如你告诉我的第一句话的信息量是3，在第一句话的基础上又告诉我一句话，额外信息量是4，那么两句话信息量加起来应该等于7。


我们再来看信息熵的公式。假设针对事件E, 每一个选择可能的概率为p1,p2,...,pn , 事件的信息熵为H。这时候计算信息熵的公式为：
$$H_{E}=-\sum_{x=1}^{n}p_{x}logp_{x}$$
我们先来看第三条信息熵随着选择增加而单调增加，而这些选择肯定是不相关的，也就是说,事件的联合概率分布是：
$$p(x_{0},x_{1}...,x_{n}) = \prod_{i=0}^{n}p(x_{i})$$
而事件信息熵是：
$$H(x_{0},x_{1}...,x_{n}) = \sum_{i=0}^{n}H(x_{i})$$
根据上面推导，再加上第二条连续变化的限制，我们很容易看出h(x)与p(x)的对数关系。

因为要确保信息熵的非负性，所以要在前面加上负号。
# 另一种关于信息熵的理解
另一个从信息编码角度的理解。信息论中，熵也代表着根据信息的概率分布对信息编码所需要的最短平均编码长度。
举个简单的例子来理解一下这件事情：假设有个考试作弊团伙，需要连续不断地向外传递4选1单选题的答案。直接传递ABCD的ascii码的话，每个答案需要8个bit的二进制编码，从传输的角度，这显然有些浪费。信息论最初要解决的，就是数据压缩和传输的问题，所以这个作弊团伙希望能用更少bit的编码来传输答案。很简单，答案只有4种可能性，所以二进制编码需要的长度就是取2为底的对数：
$$log_{2}(4)=2$$
2个bit就足够进行四个答案的编码了（00,01,10,11）。在上面这个例子中，其实隐含了一种假设，就是四个答案出现概率是相等的，均为p=1/4，所以编码需要长度的计算可以理解为如下的形式：
$$log_{2}(4)=-log_{2}(\frac{1}{4})=2$$
答案出现概率相等的例子可能并不贴近实际，在中国考试界，坊间传闻：“不知道选什么的时候就蒙C”，这个信息是可以帮助作弊团队改善编码长度的。假设A出现的概率不变仍为1/4，C出现的概率变成了1/2，B和D则分别是1/8：P(A)=1/4，P(B)=1/8，P(C)=1/2，P(D)=1/8。在这种情况下，考虑到传递答案的过程中，C出现的次数(概率)最高，所以可以为C用短一些的编码，而出现次数较少的B和D则可以考虑用长一些的编码。这样的话，平均下来，对于一定的信息总量，需要的编码总长度就会少一些。根据熵的定义的思路，对于出现概率为p的事件，考虑用长度为-log2(p)的二进制进行编码。所以考虑如下面的编码：
    
    A: 10
    B: 110
    C: 0
    D: 111

对照熵的公式来计算一下编码长度的期望值，也就是平均编码长度：
$$\sum_{x \in \{A,B,C,D\} }^{}p(x)log_{2}p(x)=\frac{1}{4}\times 2+\frac{1}{8}\times 3+\frac{1}{2}\times 1+\frac{1}{8}\times 3=1.75$$
再详细点，假设作弊团伙要传递200个答案出去。为了方便说明，这200个答案中ABCD出现的次数恰好都严格和其出现概率成比例，也就是A：50次，B：25次，C：100次，D：25次。所以传递200个答案一共需要的bit数是：那么平均下来每个答案耗费了350/200=1.75个bit编码长度。
# 信息熵的应用-决策树
机器学习中，决策树是一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表的某个可能的属性值，而每个叶结点则对应从根节点到该叶节点所经历的路径所表示的对象的值。

而信息熵则运用在每个节点如何选取特征进行分裂时。我们优先选则使信息熵下降最多的特征作为分裂节点。这个也很好理解，信息熵就是用来量化信息不确定性的，信息熵下降的越多，说明用这个特征分裂后的数据确定性增加的越多。

举个简单的例子，有一个数据集里面有两类数据，各占50%。现在房地产市场比较火爆，咱们就把这个数据集称为优选二手房数据集，为了计算简单，这个数据集一半的房子是优选，一半不是。那么这个数据集的信息熵就是：
$$H=-\sum_{i=1}^{2}p(i)log_{2}p(i) =-\frac{1}{2}\times (-1)-\frac{1}{2}\times (-1)=1$$
这时我们看到这个数据集肯定也很疑惑，凭啥有的是优选，有的就不是，什么标准，所以此时这个数据集的信息熵是最高的，因为不确定性的因素太多。

我们又得知这些二手房有五个属性，是否是好学区，是否靠近地铁站，是否是精装修，是否是2000年后的房屋，是否是满五唯一。
在这五个属性里，这些房子的概率分布如下：

    好学区：    优选0.9   非优选0.1
    近地铁：    优选0.7   非优选0.3
    精装修：    优选0.6   非优选0.4
    2000年后：  优选0.6   非优选0.4
    满五唯一：   优选0.8   非优选0.2

我们这时可以计算这五种属性各自的条件熵了。

好学区：
$$H_{好学区}=-\sum_{i=1}^{2}p(i)log_{2}p(i) =-\frac{1}{10}\times log_{2}\frac{1}{10} -\frac{9}{10}\times log_{2}\frac{9}{10}=0.4690$$
近地铁：
$$H_{近地铁}=-\sum_{i=1}^{2}p(i)log_{2}p(i) =-\frac{3}{10}\times log_{2}\frac{3}{10} -\frac{7}{10}\times log_{2}\frac{7}{10}=0.8813$$
精装修：
$$H_{精装修}=-\sum_{i=1}^{2}p(i)log_{2}p(i) =-\frac{4}{10}\times log_{2}\frac{4}{10} -\frac{6}{10}\times log_{2}\frac{6}{10}=0.9709$$
2000年后:
$$H_{2000年后}=-\sum_{i=1}^{2}p(i)log_{2}p(i) =-\frac{4}{10}\times log_{2}\frac{4}{10} -\frac{6}{10}\times log_{2}\frac{6}{10}=0.9709$$
满五唯一:
$$H_{满五唯一}=-\sum_{i=1}^{2}p(i)log_{2}p(i) =-\frac{2}{10}\times log_{2}\frac{2}{10} -\frac{8}{10}\times log_{2}\frac{8}{10}=0.7219$$
我们这是来计算属性的信息增益，就是属性包含的信息量，肯定是让信息熵减少越多的属性，信息增益越大，计算方式也很简单，就是数据的信息熵减去属性的条件熵。

信息增益：

    好学区：    0.5310
    近地铁：    0.1187
    精装修：    0.0291
    2000年后：  0.0291
    满五唯一：   0.2781

可以看到好学区的信息增益最大，也就是说选择好学区这个属性作为分裂节点，整个数据集的确定性增长的最多。
